{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX for NLP\n",
    "\n",
    "## Installing JAX on GPU\n",
    "\n",
    "To install JAX with GPU support, we first need to figure out which Python version and CUDA version are installed in our machine.\n",
    "To do so, we just need to run the following two commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\r\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\r\n",
      "Cuda compilation tools, release 10.1, V10.1.243\r\n",
      "Python 3.6.6 :: Anaconda, Inc.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the command's output, we have the following python and CUDA versions:\n",
    "- Python 3.6\n",
    "- CUDA 10.1\n",
    "\n",
    "Lastly, to install JAX, we have to retrieve its wheels with GPU support from: `https://storage.googleapis.com/jax-releases/\\${CUDA_VER}/jaxlib-0.1.39-\\${PYTHON_VER}-none-linux_x86_64.whl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jaxlib==0.1.39\r\n",
      "  Downloading https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.39-cp36-none-linux_x86_64.whl (65.5 MB)\r\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65.5 MB 52 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /opt/conda/lib/python3.6/site-packages (from jaxlib==0.1.39) (1.4.1)\r\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /opt/conda/lib/python3.6/site-packages (from jaxlib==0.1.39) (1.18.1)\r\n",
      "Requirement already satisfied, skipping upgrade: absl-py in /opt/conda/lib/python3.6/site-packages (from jaxlib==0.1.39) (0.9.0)\r\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.6/site-packages (from absl-py->jaxlib==0.1.39) (1.14.0)\r\n",
      "Installing collected packages: jaxlib\r\n",
      "  Attempting uninstall: jaxlib\r\n",
      "    Found existing installation: jaxlib 0.1.38\r\n",
      "    Uninstalling jaxlib-0.1.38:\r\n",
      "      Successfully uninstalled jaxlib-0.1.38\r\n",
      "Successfully installed jaxlib-0.1.39\r\n",
      "Requirement already up-to-date: jax in /opt/conda/lib/python3.6/site-packages (0.1.59)\r\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum in /opt/conda/lib/python3.6/site-packages (from jax) (3.1.0)\r\n",
      "Requirement already satisfied, skipping upgrade: absl-py in /opt/conda/lib/python3.6/site-packages (from jax) (0.9.0)\r\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /opt/conda/lib/python3.6/site-packages (from jax) (1.18.1)\r\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.6/site-packages (from absl-py->jax) (1.14.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade \"https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.39-cp36-none-linux_x86_64.whl\"\n",
    "!pip install --upgrade jax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done here, now we can start with the cool part ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX is running on gpu\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "print('JAX is running on', jax.lib.xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model explanation\n",
    "\n",
    "In this notebook, we are going to implement a Natural Language Processing (NLP) model based on \"Word Embeddings\".   \n",
    "\n",
    "\"Word embeddings\" are a family of natural language processing techniques aiming at mapping semantic meaning into a geometric space. This is done by associating a numeric vector to every word in a dictionary, such that the distance (e.g. L2 distance or more commonly cosine distance) between any two vectors would capture part of the semantic relationship between the two associated words. The geometric space formed by these vectors is called an embedding space (FraÃ§ois Chollet, 2016).\n",
    "\n",
    "Since this competition is a *getting-started* one, we do not have enough data to build a set of robust embeddings, hence we are going to use embeddings that were pretrained on a huge text corpus, more precisely, we are going to use the Global Vectors for Word Representation [(GloVe)](https://nlp.stanford.edu/projects/glove/) embeddings.\n",
    "\n",
    "On top of the GloVe embeddings, we are going to stack a set of 1D Convolutional layers and finally, a pair of linear or dense layers to do the classification. \n",
    "\n",
    "But wait... Does JAX provide all of those features to work with NLP? For now, it doesn't. \n",
    "\n",
    "Various python packages provide extra Deep Learning features over JAX, for example, the most popular packages nowadays are FLAX, Haiku, and STAX.\n",
    "\n",
    "In this notebook we are not going to use any extra package apart from JAX, we are going to implement all the needed Layers from scratch.\n",
    "\n",
    "## Data preprocess\n",
    "\n",
    "Before the implementation, we are going to take a quick look to the data and build our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv  test.csv  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../input/nlp-getting-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../input/nlp-getting-started/train.csv', encoding='utf-8')\n",
    "test_df = pd.read_csv('../input/nlp-getting-started/test.csv', encoding='utf-8')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to clean the data applying these simple steps:\n",
    "\n",
    "1. Remove URLs\n",
    "2. Remove HTML Tags\n",
    "3. Remove Emojis\n",
    "4. Remove punctuation\n",
    "5. Lowercase all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this # earthquake ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask . canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13 , 000 people receive # wildfires evacuation...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby # alaska as...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  our deeds are the reason of this # earthquake ...   \n",
       "1   4     NaN      NaN            forest fire near la ronge sask . canada   \n",
       "2   5     NaN      NaN  all residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13 , 000 people receive # wildfires evacuation...   \n",
       "4   7     NaN      NaN  just got sent this photo from ruby # alaska as...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_tweet(tweet: str) -> str:\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    tweet = url.sub(r'',tweet)\n",
    "    \n",
    "    html = re.compile(r'<.*?>')\n",
    "    tweet = html.sub(r'', tweet)\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "    \n",
    "    tweet = re.sub('([.,!?()#])', r' \\1 ', tweet)\n",
    "    tweet = re.sub('\\s{2,}', ' ', tweet)\n",
    "\n",
    "    return tweet.lower()\n",
    "\n",
    "df['text'] = df['text'].apply(clean_tweet)\n",
    "test_df['text'] = test_df['text'].apply(clean_tweet)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use Keras text preprocessing tools to create our vocabulary. This will help us create the vocabulary and also pad the sequences in an intuitive manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 64\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None, filters='')\n",
    "tokenizer.fit_on_texts(df.text.tolist() + test_df.text.tolist())\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(df.text.tolist())\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df.text.tolist())\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "word2idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 1),\n",
       " ('#', 2),\n",
       " ('the', 3),\n",
       " ('?', 4),\n",
       " ('a', 5),\n",
       " ('to', 6),\n",
       " ('in', 7),\n",
       " ('of', 8),\n",
       " ('and', 9),\n",
       " ('i', 10)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word2idx.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers implementation\n",
    "\n",
    "Before starting the layers implementation, it is a good practice to define a common abstraction for all our modules. You can think of Keras using the superclass `Layer` and PyTorch using the `nn.Module` class.\n",
    " For simplicity our superclass will be a named tuple called `JaxModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Container, NamedTuple\n",
    "\n",
    "Parameter = Container[np.ndarray]\n",
    "ForwardFn = Callable[[Parameter, np.ndarray, Any], np.ndarray]\n",
    "\n",
    "class JaxModule(NamedTuple):\n",
    "    # Dict or list contining the layer parameters\n",
    "    parameters: Parameter\n",
    "    \n",
    "    # How we operate with parameters to generate an output\n",
    "    forward_fn: ForwardFn\n",
    "    \n",
    "    def update(self, new_parameters) -> 'JaxModule':\n",
    "        # As tuples are immutable, we create a new jax module keeping the\n",
    "        # forward_fn but with new parameters\n",
    "        return JaxModule(new_parameters, self.forward_fn)\n",
    "    \n",
    "    def __call__(self, x: np.ndarray, **kwargs) -> np.ndarray:\n",
    "        # Automatically injects parameters\n",
    "        return self.forward_fn(self.parameters, x, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how we are going to use the `JaxModule` along the implementations, let's see how a simple linear layer would be implemented.\n",
    "\n",
    "As you may know, a linear layer follows the below equation:\n",
    "\n",
    "$ f(x) = W Â· x + b $ where W and b are trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (10, 32) Output shape after linear: (10, 512)\n"
     ]
    }
   ],
   "source": [
    "def linear(key: np.ndarray, \n",
    "           in_features: int, \n",
    "           out_features: int, \n",
    "           activation = lambda x: x) -> 'JaxModule':\n",
    "    x_init = jax.nn.initializers.xavier_normal()\n",
    "    u_init = jax.nn.initializers.uniform()\n",
    "\n",
    "    key, W_key, b_key = jax.random.split(key, 3)\n",
    "    W = x_init(W_key, shape=(in_features, out_features))\n",
    "    b = u_init(b_key, shape=(out_features,))\n",
    "    params = dict(W=W, bias=b)\n",
    "    \n",
    "    def forward_fn(params: Parameter, x: np.ndarray, **kwargs):\n",
    "        return np.dot(x, params['W']) + params['bias']\n",
    "    \n",
    "    return JaxModule(parameters=params, forward_fn=forward_fn)\n",
    "\n",
    "def flatten(key: np.ndarray) -> 'JaxModule':\n",
    "    # Reshapes the data to have 2 dims [BATCH, FEATURES]\n",
    "    def forward_fn(params, x, **kwargs):\n",
    "        bs = x.shape[0]\n",
    "        return x.reshape(bs, -1)\n",
    "    return JaxModule({}, forward_fn)\n",
    "\n",
    "\n",
    "key, subkey, linear_key = jax.random.split(key, 3)\n",
    "\n",
    "# Input vector of shape [BATCH, FEATURES]\n",
    "mock_in = jax.random.uniform(subkey, shape=(10, 32))\n",
    "linear_layer = linear(linear_key, 32, 512)\n",
    "\n",
    "print('Input shape:', mock_in.shape, \n",
    "      'Output shape after linear:', linear_layer(mock_in).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty easy, isn't it? Let's move to Convolutional 1D implementation. \n",
    "\n",
    "This one is going to be a little bit more tricky. This is because we will get a bit deeper and use with a JAX primitive. A JAX primitive it is a function that directly wraps an XLA operation. \n",
    "\n",
    "So, to develop the 1D Conv, we are going to need the primitive `conv_general_dilated`. We won't get into details on how to use this primitive here. If you are interested in it, I recommend you reading [this section](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#%F0%9F%94%AA-Convolutions) of JAX documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (10, 128, 32) Output shape after 1D Convolution: (10, 64, 512)\n"
     ]
    }
   ],
   "source": [
    "def conv_1d(key: np.ndarray, \n",
    "            in_features: int, \n",
    "            out_features: int,\n",
    "            kernel_size: int,\n",
    "            strides: int = 1,\n",
    "            padding: str = 'SAME',\n",
    "            activation = lambda x: x) -> 'JaxModule':\n",
    "    \n",
    "    # [KERNEL_WIDTH, IN_FEATURES, OUT_FEATURES]\n",
    "    kernel_shape = (kernel_size, in_features, out_features)\n",
    "    # [BATCH, WIDTH, IN_FEATURES]\n",
    "    seq_shape = (None, None, in_features)\n",
    "    \n",
    "    # Declare convolutional specs\n",
    "    dn = jax.lax.conv_dimension_numbers(\n",
    "        seq_shape, kernel_shape, ('NWC', 'WIO', 'NWC'))\n",
    "    \n",
    "    key, k_key, b_key = jax.random.split(key, 3)\n",
    "    \n",
    "    kernel = jax.nn.initializers.glorot_normal()(k_key, shape=kernel_shape)\n",
    "    b = jax.nn.initializers.uniform()(b_key, shape=(out_features,))\n",
    "    params = dict(kernel=kernel, bias=b)\n",
    "    \n",
    "    def forward_fn(params: Parameter, x: np.ndarray, **kwargs):\n",
    "        return activation(jax.lax.conv_general_dilated(\n",
    "            x, params['kernel'], \n",
    "            (strides,), padding, \n",
    "            (1,), (1,), dimension_numbers=dn) + params['bias'])\n",
    "\n",
    "    return JaxModule(params, forward_fn)\n",
    "\n",
    "\n",
    "key, subkey, conv_key = jax.random.split(key, 3)\n",
    "\n",
    "# Input vector of shape [BATCH, FEATURES]\n",
    "mock_in = jax.random.uniform(subkey, shape=(10, 128, 32))\n",
    "conv = conv_1d(conv_key, 32, 512, kernel_size=3, strides=2)\n",
    "\n",
    "print('Input shape:', mock_in.shape, \n",
    "      'Output shape after 1D Convolution:', conv(mock_in).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost done. We only have the Embedding layer implementation left. Believe me when I say that this implementation is the simplest that we are going to see today. \n",
    "\n",
    "Actually, as we won't train embeddings from scratch, we don't really need a JaxModule initialized with random weights. What we are going to do, is just load the embeddings from a GloVe file and create a matrix where each row corresponds to one embedding of our vocabulary.\n",
    "\n",
    "Surisingly, NLP Stanford GloVe webpage has a set of embedding pretrained on Twitter text corpus available to download, which is going to be pretty handy for this competion as we are dealing with tweets classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove-twitter-27b-50d  nlp-getting-started\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for \"hello\" is: [ 0.28751    0.31323   -0.29318    0.17199   -0.69232   -0.4593\n",
      "  1.3364     0.709      0.12118    0.11476   -0.48505   -0.088608\n",
      " -3.0154    -0.54024   -1.326      0.39477    0.11755   -0.17816\n",
      " -0.32272    0.21715    0.043144  -0.43666   -0.55857   -0.47601\n",
      " -0.095172   0.0031934  0.1192    -0.23643    1.3234    -0.45093\n",
      " -0.65837   -0.13865    0.22145   -0.35806    0.20988    0.054894\n",
      " -0.080322   0.48942    0.19206    0.4556    -1.642     -0.83323\n",
      " -0.12974    0.96514   -0.18214    0.37733   -0.19622   -0.12231\n",
      " -0.10496    0.45388  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as onp # Use onp to avoid overheat of moving each embedding to GPU\n",
    "\n",
    "embeddings_index = {}\n",
    "embedding_matrix = onp.zeros((len(word2idx) + 1, 50))\n",
    "\n",
    "f = open('../input/glove-twitter-27b-50d/glove.twitter.27B.50d.txt')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = onp.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embeddings = jax.device_put(embedding_matrix)\n",
    "print('Embedding for \"hello\" is:', embeddings[word2idx['hello']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a helper function to index our embedding matrix with a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (10, 128) Output shape after Embeddings: (10, 128, 50)\n"
     ]
    }
   ],
   "source": [
    "def embed_sequence(sequence: np.ndarray) -> np.ndarray:\n",
    "    return embeddings[sequence.astype('int32')]\n",
    "\n",
    "key, subkey, embedding_key = jax.random.split(key, 3)\n",
    "\n",
    "# Input vector of shape [BATCH, FEATURES]\n",
    "mock_in = jax.random.randint(subkey, \n",
    "                             minval=0,\n",
    "                             maxval=512,\n",
    "                             shape=(10, 128))\n",
    "\n",
    "print('Input shape:', mock_in.shape, \n",
    "      'Output shape after Embeddings:', embed_sequence(mock_in).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bfff... was a long journey, wasn't it? ðŸ˜© But that's all we need to do for our implementation, let's move to model training.\n",
    "\n",
    "## Training a model\n",
    "\n",
    "First of all we have to declare our model. To do so, we are going to use a similar API interface as the nn.Sequential defined in PyTorch or tf.keras.models.Sequential defined in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (10, 128) Output shape after model: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "from functools import partial\n",
    "\n",
    "# Partially evaluated layers without the random key\n",
    "PartialLayer = Callable[[np.ndarray], JaxModule]\n",
    "\n",
    "def sequential(key: np.ndarray, *modules: Sequence[PartialLayer]) -> JaxModule:\n",
    "    key, *subkeys = jax.random.split(key, len(modules) + 1)\n",
    "    model = [m(k) for k, m in zip(subkeys, modules)]\n",
    "    \n",
    "    def forward_fn(params, x, **kwargs):\n",
    "        for m, p in zip(model, params):\n",
    "            x = m.forward_fn(p, x, **kwargs)\n",
    "        return x\n",
    "    \n",
    "    return JaxModule([m.parameters for m in model], forward_fn)\n",
    "\n",
    "mock_model = sequential(\n",
    "    key,\n",
    "    partial(conv_1d, \n",
    "            in_features=50, out_features=256, \n",
    "            kernel_size=5, strides=2, activation=jax.nn.relu),\n",
    "    flatten,\n",
    "    partial(linear, \n",
    "            in_features=16384, out_features=128, \n",
    "            activation=jax.nn.relu),\n",
    "    partial(linear, \n",
    "            in_features=128, out_features=1, \n",
    "            activation=jax.nn.sigmoid))\n",
    "\n",
    "embedded_in = embed_sequence(mock_in)\n",
    "\n",
    "print('Input shape:', mock_in.shape, \n",
    "      'Output shape after model:', mock_model(embedded_in).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With JAX we can easily compute the model's weights gradients with respect an error function. Since we are dealing with a binary classification, we are going to implement the binary cross entropy loss (BCE loss). The BCE Loss follows the equation described below:\n",
    "\n",
    "$ bce = - (y * log(\\hat{y}) + (1 - y) * log(1 - \\hat{y})) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(y_hat: np.ndarray, y: np.ndarray) -> float:\n",
    "    y_hat = y_hat.reshape(-1)\n",
    "    y = y.reshape(-1)\n",
    "    y_hat = np.clip(y_hat, 1e-6, 1 - 1e-6)\n",
    "    pt = np.where(y == 1, y_hat, 1 - y_hat)\n",
    "    loss = -np.log(pt)\n",
    "    return np.mean(loss)\n",
    "\n",
    "\n",
    "def create_backward(model: JaxModule):\n",
    "    # Backward is just a function that receives the model parameters and combines them\n",
    "    # using the forward_fn. This will allow as to compute the partial derivate of the \n",
    "    # weights with respect to the loss\n",
    "    def backward(params: Sequence[Parameter], \n",
    "                 x: np.ndarray, y: np.ndarray) -> float:\n",
    "        y_hat = model.forward_fn(params, x)\n",
    "        return bce(y_hat, y)\n",
    "    return backward\n",
    "\n",
    "# Compile and differentiate the function\n",
    "backward_fn = jax.jit(jax.value_and_grad(create_backward(mock_model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are capable of computing the gradients with respect the loss, we need an optimizer to update our parameters in the correct direction so we can minimize the loss. For simplicity, we implement a basic stocastic gradient descent (SGD). The SGD update rule is as follows:\n",
    "\n",
    "`new_param = old_param - learning_rate * grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "def optimizer_step(params: Sequence[Parameter], \n",
    "                   gradients: Sequence[Parameter]) -> Sequence[Parameter]:\n",
    "    def optim_single(param, grad):\n",
    "        for p in param:\n",
    "            param[p] = param[p] - LEARNING_RATE * grad[p]\n",
    "        return param\n",
    "\n",
    "    return [optim_single(p, g) for p, g in zip(params, gradients)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it all together and try a single update on our model and see if the parameters get updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.1141462\n",
      "Loss after one step: 1.7008412\n"
     ]
    }
   ],
   "source": [
    "# Mock labels\n",
    "y_trues = np.array([0, 1, 1, 0, 0, 1, 0, 0, 1, 0])\n",
    "\n",
    "loss, grads = backward_fn(mock_model.parameters, embedded_in, y_trues)\n",
    "print('Loss:', loss)\n",
    "\n",
    "# Update the parameters with the obtained gradients\n",
    "new_params = optimizer_step(mock_model.parameters, grads)\n",
    "model = mock_model.update(new_params)\n",
    "\n",
    "loss, grads = backward_fn(mock_model.parameters, embedded_in, y_trues)\n",
    "print('Loss after one step:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to do what we already know how to do:\n",
    "\n",
    "1. Split the data to compute the metrics on a validation set and ensure that we are not overfitting the dataset\n",
    "2. Create a training loop and update the model at each training step\n",
    "3. Create the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = train_sequences\n",
    "y = df.target.values\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=.9)\n",
    "\n",
    "x_train = jax.device_put(x_train)\n",
    "x_val = jax.device_put(x_val)\n",
    "y_train = jax.device_put(y_train)\n",
    "y_val = jax.device_put(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the final model. The one that should work... ðŸ˜¯\n",
    "\n",
    "> The aim of this kernel is not to provide the best solution, it is just to guide and encourage you to use JAX and see how flexible it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sequential(\n",
    "    key,\n",
    "    partial(conv_1d, in_features=50, out_features=128,  kernel_size=7),\n",
    "    partial(conv_1d, in_features=128, out_features=128, kernel_size=7, strides=2, activation=jax.nn.relu),\n",
    "    \n",
    "    partial(conv_1d, in_features=128, out_features=256, kernel_size=5),\n",
    "    partial(conv_1d, in_features=256, out_features=256, kernel_size=5, strides=2, activation=jax.nn.relu),\n",
    "    \n",
    "    flatten,\n",
    "    partial(linear, in_features=4096, out_features=128, activation=jax.nn.relu),\n",
    "    partial(linear, in_features=128, out_features=1, activation=jax.nn.sigmoid))\n",
    "\n",
    "backward_fn = jax.jit(jax.value_and_grad(create_backward(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the train and validation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] loss: 0.7677\n",
      "-- Validation --\n",
      "Loss: 0.6951687932014465\n",
      "Accuracy: 0.610236220472441\n",
      "Recall: 0.3484848484848485\n",
      "F1-Score: 0.4364326375711575\n",
      "Epoch[1] loss: 0.6686\n",
      "-- Validation --\n",
      "Loss: 0.6256797313690186\n",
      "Accuracy: 0.6469816272965879\n",
      "Recall: 0.44242424242424244\n",
      "F1-Score: 0.5204991087344027\n",
      "Epoch[2] loss: 0.6492\n",
      "-- Validation --\n",
      "Loss: 0.6162245869636536\n",
      "Accuracy: 0.6745406824146981\n",
      "Recall: 0.48787878787878786\n",
      "F1-Score: 0.5649122807017543\n",
      "Epoch[3] loss: 0.6260\n",
      "-- Validation --\n",
      "Loss: 0.6151533126831055\n",
      "Accuracy: 0.6902887139107612\n",
      "Recall: 0.43636363636363634\n",
      "F1-Score: 0.549618320610687\n",
      "Epoch[4] loss: 0.6162\n",
      "-- Validation --\n",
      "Loss: 0.6123585104942322\n",
      "Accuracy: 0.7073490813648294\n",
      "Recall: 0.4696969696969697\n",
      "F1-Score: 0.5816135084427768\n",
      "Epoch[5] loss: 0.5930\n",
      "-- Validation --\n",
      "Loss: 0.5699965357780457\n",
      "Accuracy: 0.7125984251968503\n",
      "Recall: 0.5696969696969697\n",
      "F1-Score: 0.6319327731092437\n",
      "Epoch[6] loss: 0.5895\n",
      "-- Validation --\n",
      "Loss: 0.55873042345047\n",
      "Accuracy: 0.7257217847769029\n",
      "Recall: 0.6\n",
      "F1-Score: 0.6545454545454547\n",
      "Epoch[7] loss: 0.5682\n",
      "-- Validation --\n",
      "Loss: 0.5501565933227539\n",
      "Accuracy: 0.7322834645669292\n",
      "Recall: 0.6363636363636364\n",
      "F1-Score: 0.673076923076923\n",
      "Epoch[8] loss: 0.5786\n",
      "-- Validation --\n",
      "Loss: 0.5429054498672485\n",
      "Accuracy: 0.7322834645669292\n",
      "Recall: 0.5727272727272728\n",
      "F1-Score: 0.6494845360824743\n",
      "Epoch[9] loss: 0.5811\n",
      "-- Validation --\n",
      "Loss: 0.5282349586486816\n",
      "Accuracy: 0.7401574803149606\n",
      "Recall: 0.6454545454545455\n",
      "F1-Score: 0.6826923076923077\n",
      "Epoch[10] loss: 0.5534\n",
      "-- Validation --\n",
      "Loss: 0.5287955403327942\n",
      "Accuracy: 0.7401574803149606\n",
      "Recall: 0.5969696969696969\n",
      "F1-Score: 0.6655405405405405\n",
      "Epoch[11] loss: 0.5575\n",
      "-- Validation --\n",
      "Loss: 0.5252838134765625\n",
      "Accuracy: 0.7454068241469817\n",
      "Recall: 0.6333333333333333\n",
      "F1-Score: 0.6830065359477123\n",
      "Epoch[12] loss: 0.5419\n",
      "-- Validation --\n",
      "Loss: 0.5214242935180664\n",
      "Accuracy: 0.7414698162729659\n",
      "Recall: 0.6606060606060606\n",
      "F1-Score: 0.688783570300158\n",
      "Epoch[13] loss: 0.5368\n",
      "-- Validation --\n",
      "Loss: 0.5147722959518433\n",
      "Accuracy: 0.7611548556430446\n",
      "Recall: 0.6242424242424243\n",
      "F1-Score: 0.6936026936026936\n",
      "Epoch[14] loss: 0.5381\n",
      "-- Validation --\n",
      "Loss: 0.5101708173751831\n",
      "Accuracy: 0.7611548556430446\n",
      "Recall: 0.6454545454545455\n",
      "F1-Score: 0.7006578947368421\n",
      "Epoch[15] loss: 0.5147\n",
      "-- Validation --\n",
      "Loss: 0.5062429308891296\n",
      "Accuracy: 0.7664041994750657\n",
      "Recall: 0.6545454545454545\n",
      "F1-Score: 0.7081967213114753\n",
      "Epoch[16] loss: 0.5122\n",
      "-- Validation --\n",
      "Loss: 0.5057122111320496\n",
      "Accuracy: 0.7742782152230971\n",
      "Recall: 0.6848484848484848\n",
      "F1-Score: 0.7243589743589743\n",
      "Epoch[17] loss: 0.5004\n",
      "-- Validation --\n",
      "Loss: 0.5006107687950134\n",
      "Accuracy: 0.7742782152230971\n",
      "Recall: 0.6666666666666666\n",
      "F1-Score: 0.7189542483660131\n",
      "Epoch[18] loss: 0.5093\n",
      "-- Validation --\n",
      "Loss: 0.49730783700942993\n",
      "Accuracy: 0.7755905511811023\n",
      "Recall: 0.6636363636363637\n",
      "F1-Score: 0.7192118226600985\n",
      "Epoch[19] loss: 0.5147\n",
      "-- Validation --\n",
      "Loss: 0.4964938461780548\n",
      "Accuracy: 0.7677165354330708\n",
      "Recall: 0.6606060606060606\n",
      "F1-Score: 0.7112561174551386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "train_steps = x_train.shape[0] // BATCH_SIZE\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for step in range(train_steps):        \n",
    "        key, subkey = jax.random.split(key)\n",
    "        batch_idx = jax.random.randint(subkey, \n",
    "                                       minval=0, \n",
    "                                       maxval=x_train.shape[0],\n",
    "                                       shape=(BATCH_SIZE,))\n",
    "        \n",
    "        x_batch = embed_sequence(x_train[batch_idx])\n",
    "        y_batch = y_train[batch_idx]\n",
    "        \n",
    "        loss, grads = backward_fn(model.parameters, x_batch, y_batch)\n",
    "        running_loss += loss\n",
    "        model = model.update(optimizer_step(model.parameters, grads))\n",
    "        \n",
    "    loss_mean = running_loss / float(step)\n",
    "    print(f'Epoch[{epoch}] loss: {loss_mean:.4f}')\n",
    "        \n",
    "    predictions = model(embed_sequence(x_val))\n",
    "    loss = bce(predictions, y_val)\n",
    "    predictions = predictions > .5\n",
    "    \n",
    "    print('-- Validation --')\n",
    "    print('Loss: {}'.format(loss))\n",
    "    print('Accuracy: {}'.format(accuracy_score(y_val, predictions)))\n",
    "    print('Recall: {}'.format(recall_score(y_val, predictions)))\n",
    "    print('F1-Score: {}'.format(f1_score(y_val, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the submission file. ðŸŽ‡ðŸŽ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = test_df.id\n",
    "targets = (model(embed_sequence(test_sequences)) > .4).reshape(-1)\n",
    "pd.DataFrame(dict(id=ids, target=targets.astype('int32'))).to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
